---
title: |
 <p align="center">
  <img src="https://www.wne.uw.edu.pl/themes/wne/images/en-logo.gif"></p>  
  <h2 style="color:#660033;" align= "center">**Spatial Econometrics Final Project**</h2>
subtitle: <h4 style="color:DimGrey;" align= "center">_Spatial Analysis of Liquor Sales in Virginia and North Carolina_</h4>
author: 
  - <h5 style="color:Grey;" align="center">_Didem Paloglu, 425160_</h5>
  - <h5 style="color:Grey;" align="center">_Huseyin Can Minareci, 417121_</h5>
output:
  html_document:
    self_contained: true
    lib_dir: libs
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
    smooth_scroll: true
editor_options: 
  chunk_output_type: console
---



```{r message=FALSE, warning=FALSE, include=FALSE}
# installing necessary packages
library(kableExtra)
library(dplyr)
library(tidyverse)
library(sf)
library(tmap)
library(geojsonio)
library(sp)
library(reshape2)
library(stplanr)
library(leaflet)
library(broom)
library(spdep)
library(maptools)
library(rgdal)
library(shape)
library(RColorBrewer)
library(lmtest)

```

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```


```{r message=FALSE, warning=FALSE, include=FALSE}
#importing data 

spadata <- readOGR(dsn = ".", layer = "NCVACO Variables")

```

## 1. INTRODUCTION

### 1.1. Aim of the Project

<p style="text-align:justify;">In this project, it is aimed to model liquor demand in Virginia and North Carolina. The original study has been conducted by Mark L. Burkey. With his research, it was aimed to find answer whether the geographical restrictions on the alcohol make any effect on the alcohol-related problems. He focused arrastments because of driving under the influence of alcohol (other drugs) and tried to explain how alcohol restriction affect the DUI arrastments. The results have shown that alcohol restrictions did not make any effect on arrastments. In our project, we tried to find the answer to a different problem which is domestic violence in households. Our research question is how geographical alcohol restrictions affect the domestic violance. We constructed both traditional regression model with OLS (benchmark model) and also spatial models. </p> 


### 1.2. Information About Data Set


<p style="text-align:justify;">In this project, the data set contains 49 variables. The year of the data set is 2003. The variables give information about geographical features of states as well as demographic features. After importing data set, first we need to know the class of the data set. With `class()` function, we can say that the data set is _"SpatialPolygonsDataFrame"_. We can see the head of the data set in below table: </p>

```{r, echo=FALSE}
head(slot(spadata,"data")) %>%
  kable(digits = 2, format = "html", row.names = TRUE, caption = "Head of the Data Set") %>%
  kable_styling(bootstrap_options = c("striped"), font_size = 10) %>%
  row_spec(row = 0, color = "#660033") %>% 
  scroll_box(width = "100%")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
class(spadata)

```

<br>

<p style="text-align:justify;">Before going deep in the data set, the variables and their descriptions are also important to understand and conduct the analysis. Some variables are clear and easy to understand, but some variables are hard to interpret and we need to understand them. Therefore, all variables and necessary descriptions of the variables are provided below:</p>

All Variables:

```{r echo=FALSE}
# variable names
names(spadata)
```

Descriptions:

* **Lon & Lat:** Longitude and latitude of county centroid
* **FIPS, FIPS2:** Code for county (Federal Information Processing Standard)
* **qtystores:** Number of liquor stores in county
* **SALESPC:** Liquor Sales per capita per year, $
* **PCI:** Per capita income
* **COMM15OVP:** % of commuting over 15 minutes to work
* **COLLENRP:** % of people currently enrolled in college
* **SOMECOLLP:** % of people with “some college” or higher education level
* **ARMEDP:** % in armed forces
* **NONWHITEP:** % of nonwhite people
* **UNEMPP:** % of unemployed people
* **ENTRECP** % of employment in entertainment or recreation fields (proxy for tourism areas)
* **PUBASSTP:** % on public assistance of some sort
* **POVPOPP:** % in poverty
* **URBANP:** % of people living in urban areas
* **FOREIGNBP:** % foreign born
* **BAPTISTSP:** % of southern baptist (historically anti-alcohol)
* **ADHERENTSP:** % of adherents of any religion
* **BKGRTOMIX:** Weighted average distance from block group to nearest bar selling liquor
* **COUNTMXBV:** Count of bars selling liquor
* **MXBVSQM:** Bars per square mile
* **BKGRTOABC:** Distance for block group to nearest retail liquor outlet (“ABC stores”)
* **MXBVPPOP18:** Bars per 1,000 people 18 and older
* **DUI1802:** DUI arrests per 1,000 people 18+
* **FVPTHH02:** Offences against families and children (domestic violence) per 1,000 households
* **DC,GA, KY, MD, SC, TN, WV, VA:** Dummy variables for counties bordering other states
* **AREALANDSQ:** Area of county
* **COUNTBKGR:**  Count of block groups in county
* **TOTALPOP:** Population of county
* **POP18OV:** Population of 18+ people in county
* **LABFORCE:** number of people in labor force in county
* **HHOLDS:** number of households in county
* **POP25OV:** Population of 25+ people in county
* **POP16OV:** Population of 16+ people in county

After having information about variables, we can see the summary of the data set. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(spadata)

```
<p style="text-align:justify;">From the summary of the data set, some variables like income and population are seen as in wrong format. So, we need to convert these variables into true format (factor to numeric). </p>

```{r message=FALSE, warning=FALSE}

# converting factor variables to numeric variables

spadata$qtystores <- as.numeric(levels(spadata$qtystores))[spadata$qtystores]
spadata$PCI <- as.numeric(levels(spadata$PCI))[spadata$PCI]
spadata$COUNTMXBV <- as.numeric(levels(spadata$COUNTMXBV))[spadata$COUNTMXBV]
spadata$COUNTBKGR <- as.numeric(levels(spadata$COUNTBKGR))[spadata$COUNTBKGR]
spadata$TOTALPOP <- as.numeric(levels(spadata$TOTALPOP))[spadata$TOTALPOP]
spadata$POP18OV <- as.numeric(levels(spadata$POP18OV))[spadata$POP18OV]
spadata$LABFORCE <- as.numeric(levels(spadata$LABFORCE))[spadata$LABFORCE]
spadata$HHOLDS <- as.numeric(levels(spadata$HHOLDS))[spadata$HHOLDS]
spadata$POP25OV <- as.numeric(levels(spadata$POP25OV))[spadata$POP25OV]
spadata$POP16OV <- as.numeric(levels(spadata$POP16OV))[spadata$POP16OV]


# spadata$qtystores <- as.numeric(spadata$qtystores)
# spadata$PCI <- as.numeric(spadata$PCI)
# spadata$COUNTMXBV <- as.numeric(spadata$COUNTMXBV)
# spadata$COUNTBKGR <- as.numeric(spadata$COUNTBKGR)
# spadata$TOTALPOP <- as.numeric(spadata$TOTALPOP)
# spadata$POP18OV <- as.numeric(spadata$POP18OV)
# spadata$LABFORCE <- as.numeric(spadata$LABFORCE)
# spadata$HHOLDS <- as.numeric(spadata$HHOLDS)
# spadata$POP25OV <- as.numeric(spadata$POP25OV)
# spadata$POP16OV <- as.numeric(spadata$POP16OV)

```
Now, all the variables can be seen in the true formats in below table:

```{r echo=FALSE, message=FALSE, warning=FALSE}
str(slot(spadata,"data"))
```

<p style="text-align:justify;"> After variable transformation, the data set is ready for analysis. In the following section, some explonatory data analysis (EDA) will be conducted in order to see the distribution of important factors in the data set.</p>

### 1.3. Explonatory Data Analysis (EDA)

<p style="text-align:justify;">EDA is important tool to see the general picture of the data set. In this section, EDA has been applied as preliminary analysis. First analysis, obviously, to look at the dependent vairable in the model. Offences against families and children (domestic violence) per 1,000 households (FVPTHH02) is the dependent variable in the models for the following sections. The visualization of the domestic sales density in geographic map as below:

```{r echo=FALSE, message=FALSE, warning=FALSE}

variable<-spadata$FVPTHH02
maxy<-25
breaks<-c(0, 5, 10, 15, 20, 25) # used in the legend 
nclr<-6
plotclr<-brewer.pal(nclr, "Reds") # from the RColorBrewer package 
fillRed<-colorRampPalette(plotclr) # from the grDevices package 
colcode<-fillRed(maxy)[round(variable) + 1] # fillRed is a function 
plot(spadata, col=colcode , lwd=1,border="gray70") 

colorlegend(posy=c(0.05,0.9), posx=c(0.9,0.92), col=fillRed(maxy), zlim=c(0, maxy), zval=breaks, main.cex=0.9) # from the shape:: package

title(main="Violence Per 1,000 Households in Virginia and North Carolina")
```

<p style="text-align:justify;">In the above figure, the top map represents Virginia and bottom map represents North Carolina. When we look at the domestic violance in the counties, it is seen that the violance rates are higher in some regions in North Carolina. In general, the domestic violance is low among both counties.</p>

***

Another useful insight could be liquor sales in both county. It is one of the most important variables since it is directly reflection of the alcohol consumption. The visualision of map for two counties is below: 

```{r echo=FALSE, message=FALSE, warning=FALSE}

variable<-spadata$SALESPC
maxy<-300
breaks<-c(0, 50, 100, 150, 200, 250, 300) # used in the legend 
nclr<-7
plotclr<-brewer.pal(nclr, "Greens") # from the RColorBrewer package 
fillRed<-colorRampPalette(plotclr) # from the grDevices package 
colcode<-fillRed(maxy)[round(variable) + 1] # fillRed is a function 
plot(spadata, col=colcode , lwd=1,border="gray70") 

colorlegend(posy=c(0.05,0.9), posx=c(0.9,0.92), col=fillRed(maxy), zlim=c(0, maxy), zval=breaks, main.cex=0.9) # from the shape:: package

title(main="Liqour Sales Per Capita Per Year ($)\n in Virginia and North Carolina")
```


<p style="text-align:justify;"> We can see that highest sales have occured in East North Carolina. Moreover, for both states the liqour sales are generally low (mostly between 0-150$).</p>

***

<p style="text-align:justify;">Second analysis is about demographic structure of the states. Urban population is a good indicator to give an idea about consumer densities. Since urban areas has more people living in it, sales are directly affected from it. Therefore, next graph shows the density of urban population in both states.</p>

```{r message=FALSE, warning=FALSE, include=FALSE}
# to use ggplot() we need to convert polygon data to sf 

spadata.sf <- st_read("NCVACO Variables.shp")
spadata.sf<- st_transform(spadata.sf, CRS("+proj=longlat +datum=NAD83")) 


```

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot() +
  geom_sf(spadata.sf, mapping = aes(geometry=geometry, fill = URBANP)) +
  labs(title = "Urban Population Density in Virginia and North Carolina",
       fill = "Urban Population") +
  annotate("text", x = -83, y = 39, size = 5, 
           color = "red", alpha = 0.4, label = "Virginia") +
  annotate("text", x = -82, y = 34, size = 5, 
           color = "red", alpha = 0.4, label = "North Carolina") +
  theme(panel.background = element_blank())

```

<p style="text-align:justify;">It is shown in the map that the urbanisation rate of the Virginia is less than North Carolina. Since urban population is more in North Carolina, the alcohol sales can be affected from this, but we will see the situation in our analysis.</p>

***

<p style="text-align:justify;">Another useful insight about the data set, mapping the another crime related indicator in the data set. In below map, we visualized DUI arrests per 1,000 people who are older than 18. Driving under the influence (DUI) is the offense of driving, operating, or being in control of a vehicle while impaired by alcohol or other drugs (including recreational drugs and those prescribed by physicians), to a level that renders the driver incapable of operating a motor vehicle safely (Source:Wikipedia). It is useful to see DUI arrests in order to have an idea about domestic violance.</p>

```{r echo=FALSE, message=FALSE, warning=FALSE}
variable<-spadata$DUI1802
maxy<-35
breaks<-c(0, 5, 10, 15, 20, 25, 30,35) # used in the legend 
nclr<-8
plotclr<-brewer.pal(nclr, "Purples") # from the RColorBrewer package 
fillRed<-colorRampPalette(plotclr) # from the grDevices package 
colcode<-fillRed(maxy)[round(variable) + 1] # fillRed is a function 
plot(spadata, col=colcode , lwd=1,border="gray70") 

colorlegend(posy=c(0.05,0.95), posx=c(0.9,0.92), col=fillRed(maxy), zlim=c(0, maxy), zval=breaks, main.cex=0.9) # from the shape:: package

title(main="Arrastment Density from DUI in\n Virginia and North Carolina")

```

<p style="text-align:justify;">From the map, it is seen that the DUI arrests are higher in North Carolina.</p> 

*** 

<p style="text-align:justify;">After getting main insight about important indicators, the next section introduce modelling of the data set. We will first construct a traditonal OLS model as benchmark. After OLS, acording to diagnostic test results, we will construct spatial models. </p>


## 2. MODELING

<p style="text-align:justify;">Before jumping construction of models, first we need a spatial weight matrix for spatial models in the following part of the model. </p>

### 2.1. Preparing Spatial Weights Matrix

<p style="text-align:justify;">Structure of the neighborhood is key factor while constructing a spatial model. In our analysis, we tried to create few neighborhood matrix which are contiguity matrix and k nearest neighbors (knn) matrix. We also checked neighbours for symmetry. </p>

The result of the contiguity matrix is provided below: 

```{r include=FALSE}
cont.nb<-poly2nb(as(spadata, "SpatialPolygons"))
cont.listw<-nb2listw(cont.nb, style="W")

# coordinates of units 
#(geometric center of gravity)
crds<-coordinates(spadata)
colnames(crds)<-c("cx", "cy")

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
cont.listw # summary of matrix

```

The plot of neighbours is provided below: 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# plot of neighbourhood
plot(spadata) # contour map
plot(cont.nb, crds, add=TRUE)
```

<p style="text-align:justify;">Secondly, we created spatial weights matrix with k nearest neighbours. The number of closest neighbours is selected 3. The result map is provided below:</p>

```{r echo=FALSE}
spa.knn<-knearneigh(crds, k=3) # knn class (k=3)
spa.knn.nb<-knn2nb(spa.knn)
plot(spadata)
plot(spa.knn.nb, crds, add=TRUE)

```

<p style="text-align:justify;">We can check whether the neighbourhood matrix is symmetric. The symmetry test shows that, the matrix is not symmetric since the test result gives FALSE. We can turn it to syymetric matrix as below:</p>

```{r message=FALSE, warning=FALSE}

# print(is.symmetric.nb(spa.knn.nb)) --> checking symmetry of matrix

# creating symettric knn neighbours matrix

spa.sym.knn.nb<-make.sym.nb(spa.knn.nb)
# print(is.symmetric.nb(spa.sym.knn.nb))

```

<p style="text-align:justify;">We can also save symmetric matrix as **listw** class. The results below shows that average number of links is nearly 3.6.</p>

```{r echo=FALSE, message=FALSE, warning=FALSE}
# knn as listw class
spa.sym.knn.listw<-nb2listw(spa.sym.knn.nb)
spa.sym.knn.listw

```

<p style="text-align:justify;">Lastly, we can calculate at what distance all units have at least one neighbour. At 0.7975231 km, all units have at least one neighbour. This neighborhood is concentrated in eastern Virginia.</p>

```{r echo=FALSE, message=FALSE, warning=FALSE}
#  k=3, list of closests neighbours
kkk<-knn2nb(knearneigh(crds, k=3)) 

# max of distances between clostests neighbours (0.7975231)
all<-max(unlist(nbdists(kkk, crds))) 

# neighbours in radius of 0.7975231 km 

all.nb<-dnearneigh(crds, 0, all) 

plot(spadata, border="grey") 
plot(all.nb, crds, add=TRUE)
```

<p style="text-align:justify;">After calculating spatial weights matrix and showing neighbourhood, it is time to construct different models.</p>

### 2.2. OLS 

<p style="text-align:justify;">First step is constructing a non-spatial model with OLS. After model construction,we will test its residuals against spatial autocorrelation by Moran's I test. Moran's I is a measure of spatial autocorrelation–how related the values of a variable are based on the locations where they were measured. </p>

<p style="text-align:justify;">The base model is constructed with variables that are thought to be important for domestic violance. The model is provided below: </p>

```{r}

ols <- lm(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata)

```

The output of the model is provided below: 

```{r echo=FALSE, warning=FALSE}
summary(ols)
```


<p style="text-align:justify;">From the result of the model, DUI1802, BKGRTOABC, BAPTISTSP, ENTRECP and URBANP (significant in 0.1 significance level) variables are seen as significant variables. This shows that educaiton, income and employement related indicators has no effect on the domestic violance. Moreover, liquor sales is insignificant in the model. It implies that alcohol consumption does not have an effect on violance.</p>


<p style="text-align:justify;">Now, we will test our model residuals' autocorrelation with Moran's I test. In addition to Moran's tests, Lagrange Multiplier test is also provided below. It is important to see the significance or misspecification of the constructed models. </p>

#### 2.2.1. Moran Correlation Test

H0: No spatial correlation in the residuals

```{r message=FALSE, warning=FALSE}

# Moran's I test
ols_I <- lm.morantest(ols, cont.listw)
print(ols_I)
```

<p style="text-align:justify;">Moran’s I statistic is significant (p < 0.05) Moran's Statistic is ~0.075 which means that there is a significant evidence that positive spatial correlation exists between the residuals. So, we should use a spatial model.</p>

#### 2.2.2. Lagrange Multiplier Test

```{r message=FALSE, warning=FALSE}

#Lagrange Multiplier Test

ols_LM <- lm.LMtests(ols, cont.listw, test = "all")
print(ols_LM)
```

<p style="text-align:justify;">The LM test results shows the significance of linear model and its lagged model, their robust version and SARMA model. When we look at the linear model significance, since LMerr and LMlag are both statistically significant different from zero, we need to look at their robust counterparts. These robust counterparts are actually robust to the presence of the other “type” of autocorrelation. The robust version of the tests suggest that the lag model is the more likely alternative.</p>


#### 2.2.3. Other Diagnostic Tests for OLS

##### 2.2.3.1. Breusch-Pagan Test for Heteroscedasticity
```{r}

bptest(ols) # heteroscedasticity (H1) /homoscedastic (H0)

```


* p-value is 0.03 so we reject H0 homoscedasticity which means we have heteroscedasticity.

##### 2.2.3.2. Ramsey’s Test for Functional Form

H1: when non-linear variables (like powers of the variables) should be included then model is mis-specified

```{r message=FALSE, warning=FALSE}
resettest(ols, power=2, type="regressor") 	
```


* p-value is 0.03152 so we reject H0, This indicates that the functional form is correct and our model does not suffer from omitted variables.

##### 2.2.3.3. Moran Scatter Plot

```{r echo=FALSE, message=FALSE, warning=FALSE}
# preparing the data
x<-spadata$FVPTHH02 # variable selection
zx<-as.data.frame(scale(x))  #standardization of variable

# Moran scatterplot – automatic version
moran.plot(zx$V1, cont.listw, pch=19, labels=as.character(spadata$NAME))

```

<p style="text-align:justify;">We can see on the Moran scatter plot how our dependent variable's spatial correlation. We can also cluster negatives and positives next to each other and check how they correlate. In order to do that we took the residuals and cluster them and after that we used join count test.</p>

```{r echo=FALSE, message=FALSE, warning=FALSE}
# test join.count for residuals (positive vs. negative)
res<-ols$residuals
resid<-factor(cut(res, breaks=c(-200, 0, 200), labels=c("negative","positive")))
joincount.test(resid, cont.listw)
```

<p style="text-align:justify;">We can see from the results that both negative and positive residuals have positive spatial correlation. Which another indicator that we should continue with spatial model instead of OLS.</p>

On the next plot we can see how the OLS residuals distribute on our map.


```{r echo=FALSE, message=FALSE, warning=FALSE}

# spatial distribution of OLS residuals
summary(ols$residuals)
res<-ols$residuals
brks<-c(min(res), mean(res)-sd(res), mean(res), mean(res)+sd(res), max(res))
cols<-c("steelblue4","lightskyblue","thistle1","plum3")
plot(spadata, col=cols[findInterval(res,brks)])
#plot(woj, add=TRUE, lwd=2)
title(main="Spatial Distribution of OLS Residuals")
legend("bottomleft", legend=c("<mean-sd", "(mean-sd, mean)", "(mean, mean+sd)", ">mean+sd"), leglabs(brks1), fill=cols, bty="n")

```


Based on Moran I test and Join Count Test we decided to use spatial models because according to both tests there is a significant evidence that there is spatial correlation between residuals.

***

### 2.3. SAR /SEM 


GNS: Y= \rho WY+Xβ+WX\theta+u and u= \lambda Wu+e

rho spatial lag of dependent variable (impacts)
beta 
theta spatial lag of explanotary variable
lambda spatial lag of error term (in order to get random term which is epsilon)

We wanted to start with Monski model to see what we should expect from our model.

### 2.4. Monski Model

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Estimation of spatial models with 3,2, or 1 spatial components

# Manski model (full specification) - includes spatial lag of Y (rho), 
# spatial lag of X (theta), spatial error term (lambda)
# option type="sacmixed" activates spatial lags of X
GNS_1<-sacsarlm(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata, listw=cont.listw, type="sacmixed", method="LU")  # method="LU" speeds up computations
summary(GNS_1)
```


DUI1802, ENTRECP significant while the rest are insignificant and if it comes to thetas all of them are insignificant. We can also see from the result that Rho and Lambda are insignificant as well and model's AIC is 1156.2, which we are seeking for the smallest AIC for the best model. 


```{r message=FALSE, warning=FALSE}

# SAC / SARAR model - includes spatial lag of Y, spatial error term
SAC_1<-sacsarlm(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata, listw=cont.listw)
summary(SAC_1)
```

As we can see in SAC / SARAR model model thetas disappeared from result as planned. DUI1802, BKGRTOABC, BAPTISTSP ENTRECP are significant. And we can see this time that both rho and lambda are significant. Rho: 0.45565 Lambda: -0.37713 And AIC: 1144.8 which is smaller than AIC's of Monski model and also smaller than AIC for lm. It means our model is getting better and for our dataset this model perform very well. We will continue to check the rest models to see if we can improve our results.


<!-- # SEM - spatial error model -->
<!-- # typically includes spatial error term only (with lambda coefficient) -->
<!-- # option etype="emixed" activates spatial lags of X (with theta coeff.) # what makes spatial Durbin error model -->
```{r message=FALSE, warning=FALSE}
SDEM_1<-errorsarlm(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata, listw=cont.listw, etype="emixed") # with spatial lags of X
summary(SDEM_1)
```

In this thetas came back and we omit rho this time. This model has AIC: 1154.8 which is bigger than SAC / SARAR Model which means it is worst than previous model. So we decided to move forward.


```{r message=FALSE, warning=FALSE}
SEM_1<-errorsarlm(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata, listw=cont.listw) # no spat-lags of X
summary(SEM_1)
```

In SEM - spatial error model we eliminate both rho and theta and lambda is significant. And from results we can see that this model has AIC: 1149 which is bigger that SAC / SARAR Model's AIC.

<!-- # SAR - spatial lag model -->

```{r message=FALSE, warning=FALSE}
SDM_1<-lagsarlm(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata, listw=cont.listw, type="mixed") # with spatial lags of X
summary(SDM_1)
```
SAR - spatial lag model normally includes spatial lag of Y only (with rho coefficient) but with option type="mixed" we activated spatial lags of X (with theta coeff.) And AIC is 1154.4 which is bigger than SAC / SARAR Model.

```{r message=FALSE, warning=FALSE}
SAR_1<-lagsarlm(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata, listw=cont.listw) # no spatial lags of X
summary(SAR_1)
```
Same model without theta coefficants performed better with AIC 1145.9 as well but still it is slightly bigger than SAC / SARAR model's AIC which is 1144.8.


<!-- # from errorsarlm() library -->
<!-- # an ‘lm’ model augmented with the spatially lagged RHS variables -->
<!-- # RHS variables – right-hand side variables -->
```{r message=FALSE, warning=FALSE}
SLX_1<-lmSLX(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata,
             listw=cont.listw)
summary(SLX_1)
```
In here we checked an ‘lm’ model augmented with the spatially lagged RHS variables which didn't perform well.


***

After seeing all model results we would like to choose our final model as SAC / SARAR model but in order to do so we need to do proper Likelihood ratio test first to see if it is indeed would be a good choice.


### 2.5. Diagnostic Tests

<!-- # LR (likelihood ratio) test - compares nested restricted model -->
<!-- # H0 – restricted (narrower) model is better -->
<!-- # H1 – unrestricted (wider) model is better -->
<!-- # df in chi2 is the number of restricted parameters -->
```{r message=FALSE, warning=FALSE}
LR.sarlm(GNS_1, SAC_1)
LR.sarlm(SAC_1, SAR_1)
LR.sarlm(SAC_1, SEM_1)

```

In the first test we compare Manski model with SAC / SARAR model and we cannot reject H0 with p-value 0.5661 which means narrower model is better and in this case SAC / SAR model is narrower.  

In the second test we compare SAC / SAR model with SARAR (Spatial Lag Model) and our p-value is 0.07981 and we think that we can reject H0 since it is below 0.1 significant level which means wider model is better and in this case SAC / SAR model is wider.

In the last test we compare SAC / SARAR model with SEM (Spatial Error Model) and we reject H0 with p-value 0.01292 which means wider model is better and in this case SAC / SAR model is wider.


### 2.6 Final Model

SAC / SARAR model - includes spatial lag of Y, spatial error term


```{r message=FALSE, warning=FALSE}

# SAC / SARAR model - includes spatial lag of Y, spatial error term
SAC_1<-sacsarlm(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata, listw=cont.listw)
summary(SAC_1)
```

As we can see in SAC / SARAR model model thetas disappeared from result as planned. DUI1802, BKGRTOABC, BAPTISTSP and ENTRECP are significant. And we can see this time that both rho and lambda are significant. Rho: 0.45565 Lambda: -0.37713 And AIC: 1144.8 which is smaller than AIC's of Monski model and also smaller than AIC for lm. It means our model is getting better and for our dataset this model perform very well. 

## 3. CONCLUSION

<p style="text-align:justify;">In this project, we tried to find the model to answer our research question that is how alcohol restrictions affect the offense related problems. First, we investigated the data set visually. After that, we started to build model model with a non-spatial one. First model was OLS. Moran's test for the OLS model showed that there is spatial effects of the residuals and spatial models should be developed. After that, we started to build different spatial models. We built Monski Model in the beginning and then SEM,SAR,SAC,SAR,SARAR models and tried to reach best model with highest significant variables and lowest AIC values. In the final stage, the best model is obtained with SAC / SARAR including spatial lag of Y, spatial error term. The main intepratation from this model output is that, domestic violance is not affected by alcohol consumption or alcohol sales. Therefore, we verified the previous study.</p>


