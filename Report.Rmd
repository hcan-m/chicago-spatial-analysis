---
title: |
 <p align="center">
  <img src="https://www.wne.uw.edu.pl/themes/wne/images/en-logo.gif"></p>  
  <h2 style="color:#660033;" align= "center">**Spatial Econometrics Final Project**</h2>
subtitle: <h4 style="color:DimGrey;" align= "center">_Spatial Analysis of Liquor Sales in Virginia and North Carolina_</h4>
author: 
  - <h5 style="color:Grey;" align="center">_Didem Paloglu, 425160_</h5>
  - <h5 style="color:Grey;" align="center">_Huseyin Can Minareci, 417121_</h5>
output:
  html_document:
    self_contained: true
    lib_dir: libs
    theme: spacelab
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
    smooth_scroll: true
editor_options: 
  chunk_output_type: console
---



```{r message=FALSE, warning=FALSE, include=FALSE}
# installing necessary packages
library(kableExtra)
library(dplyr)
library(tidyverse)
library(sf)
library(tmap)
library(geojsonio)
library(sp)
library(reshape2)
library(stplanr)
library(leaflet)
library(broom)
library(spdep)
library(maptools)
library(rgdal)
library(shape)
library(RColorBrewer)
library(lmtest)

```

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```


```{r message=FALSE, warning=FALSE, include=FALSE}
#importing data 

spadata <- readOGR(dsn = ".", layer = "NCVACO Variables")

```

## 1. INTRODUCTION

### 1.1. Aim of the Project

<p style="text-align:justify;">In this project, it is aimed to model liquor demand in Virginia and North Carolina. The original study has been conducted by Mark L. Burkey. With his research, it was aimed to find answer whether the geographical restrictions on the alcohol make any effect on the alcohol-related problems. The results have shown that alcohol restrictions did not make any effect on alcohol-related problems. In our project, we tried to find the answer to our research question: Which factors affect the alcohol demand and how geographical alcohol restrictions changes consumer behaviours. We constructed both traditional regression model with OLS (benchmark model) and also spatial models with SAR & SEM. </p> 


### 1.2. Information About Data Set


<p style="text-align:justify;">In this project, the data set contains 49 variables. The year of the data set is 2003. The variables give information about geographical features of states as well as demographic features. After importing data set, first we need to know the class of the data set. With `class()` function, we can say that the data set is _"SpatialPolygonsDataFrame"_. We can see the head of the data set in below table: </p>

```{r, echo=FALSE}
head(slot(spadata,"data")) %>%
  kable(digits = 2, format = "html", row.names = TRUE, caption = "Head of the Data Set") %>%
  kable_styling(bootstrap_options = c("striped"), font_size = 10) %>%
  row_spec(row = 0, color = "#660033") %>% 
  scroll_box(width = "100%")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
class(spadata)

```

<br>

<p style="text-align:justify;">Before going deep in the data set, the variables and their descriptions are also important to understand and conduct the analysis. Some variables are clear and easy to understand, but some variables are hard to interpret and we need to understand them. Therefore, all variables and necessary descriptions of the variables are provided below:</p>

All Variables:

```{r echo=FALSE}
# variable names
names(spadata)
```

Descriptions:

* **Lon & Lat:** Longitude and latitude of county centroid
* **FIPS, FIPS2:** Code for county (Federal Information Processing Standard)
* **qtystores:** Number of liquor stores in county
* **SALESPC:** Liquor Sales per capita per year, $
* **PCI:** Per capita income
* **COMM15OVP:** % of commuting over 15 minutes to work
* **COLLENRP:** % of people currently enrolled in college
* **SOMECOLLP:** % of people with “some college” or higher education level
* **ARMEDP:** % in armed forces
* **NONWHITEP:** % of nonwhite people
* **UNEMPP:** % of unemployed people
* **ENTRECP** % of employment in entertainment or recreation fields (proxy for tourism areas)
* **PUBASSTP:** % on public assistance of some sort
* **POVPOPP:** % in poverty
* **URBANP:** % of people living in urban areas
* **FOREIGNBP:** % foreign born
* **BAPTISTSP:** % of southern baptist (historically anti-alcohol)
* **ADHERENTSP:** % of adherents of any religion
* **BKGRTOMIX:** Weighted average distance from block group to nearest bar selling liquor
* **COUNTMXBV:** Count of bars selling liquor
* **MXBVSQM:** Bars per square mile
* **BKGRTOABC:** Distance for block group to nearest retail liquor outlet (“ABC stores”)
* **MXBVPPOP18:** Bars per 1,000 people 18 and older
* **DUI1802:** DUI arrests per 1,000 people 18+
* **FVPTHH02:** Offences against families and children (domestic violence) per 1,000 households
* **DC,GA, KY, MD, SC, TN, WV, VA:** Dummy variables for counties bordering other states
* **AREALANDSQ:** Area of county
* **COUNTBKGR:**  Count of block groups in county
* **TOTALPOP:** Population of county
* **POP18OV:** Population of 18+ people in county
* **LABFORCE:** number of people in labor force in county
* **HHOLDS:** number of households in county
* **POP25OV:** Population of 25+ people in county
* **POP16OV:** Population of 16+ people in county

After having information about variables, we can see the summary of the data set. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(spadata)

```
<p style="text-align:justify;">From the summary of the data set, some variables like income and population are seen as in wrong format. So, we need to convert these variables into true format (factor to numeric). </p>

```{r message=FALSE, warning=FALSE}

# converting factor variables to numeric variables

spadata$qtystores <- as.numeric(levels(spadata$qtystores))[spadata$qtystores]
spadata$PCI <- as.numeric(levels(spadata$PCI))[spadata$PCI]
spadata$COUNTMXBV <- as.numeric(levels(spadata$COUNTMXBV))[spadata$COUNTMXBV]
spadata$COUNTBKGR <- as.numeric(levels(spadata$COUNTBKGR))[spadata$COUNTBKGR]
spadata$TOTALPOP <- as.numeric(levels(spadata$TOTALPOP))[spadata$TOTALPOP]
spadata$POP18OV <- as.numeric(levels(spadata$POP18OV))[spadata$POP18OV]
spadata$LABFORCE <- as.numeric(levels(spadata$LABFORCE))[spadata$LABFORCE]
spadata$HHOLDS <- as.numeric(levels(spadata$HHOLDS))[spadata$HHOLDS]
spadata$POP25OV <- as.numeric(levels(spadata$POP25OV))[spadata$POP25OV]
spadata$POP16OV <- as.numeric(levels(spadata$POP16OV))[spadata$POP16OV]

# 
# spadata$qtystores <- as.numeric(spadata$qtystores)
# spadata$PCI <- as.numeric(spadata$PCI)
# spadata$COUNTMXBV <- as.numeric(spadata$COUNTMXBV)
# spadata$COUNTBKGR <- as.numeric(spadata$COUNTBKGR)
# spadata$TOTALPOP <- as.numeric(spadata$TOTALPOP)
# spadata$POP18OV <- as.numeric(spadata$POP18OV)
# spadata$LABFORCE <- as.numeric(spadata$LABFORCE)
# spadata$HHOLDS <- as.numeric(spadata$HHOLDS)
# spadata$POP25OV <- as.numeric(spadata$POP25OV)
# spadata$POP16OV <- as.numeric(spadata$POP16OV)

```
Now, all the variables can be seen in the true formats in below table:

```{r echo=FALSE, message=FALSE, warning=FALSE}
str(slot(spadata,"data"))
```

<p style="text-align:justify;"> After variable transformation, the data set is ready for analysis. In the following section, some explonatory data analysis (EDA) will be conducted in order to see the distribution of important factors in the data set.</p>

### 1.3. Explonatory Data Analysis (EDA)

<p style="text-align:justify;">EDA is important tool to see the general picture of the data set. In this section, EDA has been applied as preliminary analysis. First analysis, obviously, to look at the dependent vairable in the model. Liqour sales (SALESPC) is the dependent variable in the data set. The visualization of the liqour sales density in geographic map as below:</p>

```{r echo=FALSE, message=FALSE, warning=FALSE}

variable<-spadata$SALESPC
maxy<-300
breaks<-c(0, 50, 100, 150, 200, 250, 300) # used in the legend 
nclr<-7
plotclr<-brewer.pal(nclr, "Reds") # from the RColorBrewer package 
fillRed<-colorRampPalette(plotclr) # from the grDevices package 
colcode<-fillRed(maxy)[round(variable) + 1] # fillRed is a function 
plot(spadata, col=colcode , lwd=1,border="gray70") 

colorlegend(posy=c(0.05,0.9), posx=c(0.9,0.92), col=fillRed(maxy), zlim=c(0, maxy), zval=breaks, main.cex=0.9) # from the shape:: package

title(main="Liqour Sales Per Capita Per Year ($)\n in Virginia and North Carolina")
```

<br> 

<p style="text-align:justify;">From the figure, the top map represents Virginia and bottom map represents North Carolina. We can see that highest sales have occured in East North Carolina. Moreover, for both states the liqour sales are generally low (mostly between 0-150$).</p>

***

<p style="text-align:justify;">Second analysis is about demographic structure of the states. Urban population is a good indicator to give an idea about consumer densities. Since urban areas has more people living in it, sales are directly affected from it. Therefore, next graph shows the density of urban population in both states.</p>

```{r message=FALSE, warning=FALSE, include=FALSE}
# to use ggplot() we need to convert polygon data to sf 

spadata.sf <- st_read("NCVACO Variables.shp")
spadata.sf<- st_transform(spadata.sf, CRS("+proj=longlat +datum=NAD83")) 


```

```{r echo=FALSE, message=FALSE, warning=FALSE}
ggplot() +
  geom_sf(spadata.sf, mapping = aes(geometry=geometry, fill = URBANP)) +
  labs(title = "Urban Population Density in Virginia and North Carolina",
       fill = "Urban Population") +
  annotate("text", x = -83, y = 39, size = 5, 
           color = "red", alpha = 0.4, label = "Virginia") +
  annotate("text", x = -82, y = 34, size = 5, 
           color = "red", alpha = 0.4, label = "North Carolina") +
  theme(panel.background = element_blank())

```

<p style="text-align:justify;">It is shown in the map that the urbanisation rate of the Virginia is less than North Carolina. Since urban population is more in North Carolina, the alcohol sales can be affected from this, but we will see the situation in our analysis.</p>

***

<p style="text-align:justify;">Another useful insight about the data set, mapping the offense related indicators. In below map, we visualized DUI arrests per 1,000 people who are older than 18. Driving under the influence (DUI) is the offense of driving, operating, or being in control of a vehicle while impaired by alcohol or other drugs (including recreational drugs and those prescribed by physicians), to a level that renders the driver incapable of operating a motor vehicle safely (Source:Wikipedia). It is useful to see DUI arrests in order to interpret the alcohol sales.</p>

```{r echo=FALSE, message=FALSE, warning=FALSE}
variable<-spadata$DUI1802
maxy<-35
breaks<-c(0, 5, 10, 15, 20, 25, 30,35) # used in the legend 
nclr<-8
plotclr<-brewer.pal(nclr, "Purples") # from the RColorBrewer package 
fillRed<-colorRampPalette(plotclr) # from the grDevices package 
colcode<-fillRed(maxy)[round(variable) + 1] # fillRed is a function 
plot(spadata, col=colcode , lwd=1,border="gray70") 

colorlegend(posy=c(0.05,0.95), posx=c(0.9,0.92), col=fillRed(maxy), zlim=c(0, maxy), zval=breaks, main.cex=0.9) # from the shape:: package

title(main="Arrastment Density from DUI in\n Virginia and North Carolina")

```

<p style="text-align:justify;">From the map, it is seen that the DUI arrests are higher in North Carolina. It could said that alcohol sales may be higher in North Carolina and we will also see the significance of DUI arrasments for alcohol sales.</p> 

*** 

<p style="text-align:justify;">After getting main insight about important indicators, the next section introduce modelling of the data set. We will first construct a traditonal OLS model as benchmark. After OLS, acording to diagnostic test results, we will construct spatial models. </p>


## 2. MODELING

<p style="text-align:justify;">Before jumping construction of models, first we need a spatial weight matrix for spatial models in the following part of the model. </p>

### 2.1. Preparing Spatial Weights Matrix

<p style="text-align:justify;">Structure of the neighborhood is key factor while constructing a spatial model. In our analysis, we tried to create few neighborhood matrix which are contiguity matrix and k nearest neighbors (knn) matrix. We also checked neighbours for symmetry. </p>

The result of the contiguity matrix is provided below: 

```{r include=FALSE}
cont.nb<-poly2nb(as(spadata, "SpatialPolygons"))
cont.listw<-nb2listw(cont.nb, style="W")

# coordinates of units 
#(geometric center of gravity)
crds<-coordinates(spadata)
colnames(crds)<-c("cx", "cy")

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
cont.listw # summary of matrix

```

The plot of neighbours is provided below: 

```{r echo=FALSE, message=FALSE, warning=FALSE}
# plot of neighbourhood
plot(spadata) # contour map
plot(cont.nb, crds, add=TRUE)
```

<p style="text-align:justify;">Secondly, we created spatial weights matrix with k nearest neighbours. The number of closest neighbours is selected 3. The result map is provided below:</p>

```{r echo=FALSE}
spa.knn<-knearneigh(crds, k=3) # knn class (k=3)
spa.knn.nb<-knn2nb(spa.knn)
plot(spadata)
plot(spa.knn.nb, crds, add=TRUE)

```

<p style="text-align:justify;">We can check whether the negihbourhood matrix is symmetric. The symmetry test shows that, the matrix is not symmetric since the test result gives FALSE. We can turn it to syymetric matrix as below:</p>

```{r message=FALSE, warning=FALSE}

# print(is.symmetric.nb(spa.knn.nb)) --> checking symmetry of matrix

# creating symettric knn neighbours matrix

spa.sym.knn.nb<-make.sym.nb(spa.knn.nb)
print(is.symmetric.nb(spa.sym.knn.nb))

```

<p style="text-align:justify;">We can also save symmetric matrix as **listw** class. The results below shows that average number of links is nearly 3.6.</p>

```{r echo=FALSE, message=FALSE, warning=FALSE}
# knn as listw class
spa.sym.knn.listw<-nb2listw(spa.sym.knn.nb)
spa.sym.knn.listw

```

<p style="text-align:justify;">Lastly, we can calculate at what distance all units have at least one neighbour. At 0.7975231 km, all units have at least one neighbour. This neighborhood is concentrated in eastern Virginia.</p>

```{r echo=FALSE, message=FALSE, warning=FALSE}
# implicitly k=3, list of closests neighbours
kkk<-knn2nb(knearneigh(crds, k=3)) 

# max of distances between clostests neighbours (0.7975231)
all<-max(unlist(nbdists(kkk, crds))) 

# neighbours in radius of 0.7975231 km 

all.nb<-dnearneigh(crds, 0, all) 

plot(spadata, border="grey") 
plot(all.nb, crds, add=TRUE)
```

<p style="text-align:justify;">After calculating spatial weights matrix and showing neighbourhood, it is time to construct different models.</p>

### 2.2. OLS 

<p style="text-align:justify;">First step is constructing a non-spatial model with OLS. After model construction,we will test its residuals against spatial autocorrelation by Moran's I test. Moran's I is a measure of spatial autocorrelation–how related the values of a variable are based on the locations where they were measured. </p>

<p style="text-align:justify;">The base model is constructed with variables that are thought to be important for alcohol sales. The model is provided below: </p>



GÜNNCEL MODEEEL!!! 
```{r}

ols <- lm(FVPTHH02 ~ SALESPC + DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP, data = spadata)

```

The output of the model is provided below: 

```{r echo=FALSE, warning=FALSE}
summary(ols)
```

<p style="text-align:justify;">From the result of the model, DUI1802 (significant for significance level 0.1), BKGRTOABC, ENTRECP, UNEMPP variables are seen as significant variables. This shows that urban population has no effect on the alcohol sales. Moreover, The higher education, number of southern baptists, weighted average distance to nearest bar selling liquor, income per capita, and offense against families and children do not have a significant effect on liquor sales. R-squared is 0.4572, which is quite okay.</p>


<p style="text-align:justify;">Now, we will test our model residuals' autocorrelation with Moran's I test. In addition to Moran's tests, Lagrange Multiplier test is also provided below. It is important to see the significance or misspecification of the constructed models. </p>

#### 2.2.1. Moran Correlation Test

H0: No spatial correlation in the residuals

```{r message=FALSE, warning=FALSE}

# Moran's I test
ols_I <- lm.morantest(ols, cont.listw)
print(ols_I)
```

Moran’s I statistic is not significant (p >0.05) Moran's Statistic is ~0.033 which means that there is a insignificant evidence that positive spatial correlation exists between the residuals and we should use a spatial model(!!!!). So, it means that spatial autocorrelation does not exist. We can keep OLS.(!!!) 

#### 2.2.2. Lagrange Multiplier Test

```{r message=FALSE, warning=FALSE}

#Lagrange Multiplier Test

ols_LM <- lm.LMtests(ols, cont.listw, test = "all")
print(ols_LM)
```

The LM test results shows the significance of linear model and its lagged model, their robust version and SARMA model. When we look at the linear model significance, the robust linear model with lag is better than classic linear model (and with lag). Also, SARMA model is significant according to test result. 

#### 2.2.3. Other Diagnostic Tests for OLS

##### 2.2.3.1. Breusch-Pagan Test for Heteroscedasticity
```{r}

bptest(ols) # heteroscedasticity (H1) /homoscedastic (H0)

```

P-value is 0.001 so we reject H0 homoscedasticity which means we have heteroscedasticity.

##### 2.2.3.2. Ramsey’s Test for Functional Form

H1: when non-linear variables (like powers of the variables) should be included then model is mis-specified

```{r}
resettest(ols, power=2, type="regressor") 	
```

p-value is 0.02517 so we reject H0, This indicates that the functional form is correct and our model does not suffer from omitted variables.

##### 2.2.3.3. Moran Scatter Plot

```{r echo=FALSE, message=FALSE, warning=FALSE}
# preparing the data
x<-spadata$SALESPC # variable selection
zx<-as.data.frame(scale(x))  #standardization of variable

# Moran scatterplot – automatic version
moran.plot(zx$V1, cont.listw, pch=19, labels=as.character(spadata$NAME))

```

We can see on the Moran scatter plot how our dependent variable's spatial correlation. We can also cluster negatives and positives next to each other and check how they correlate. In order to do that we took the residuals and cluster them and after that we used join count test.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# test join.count for residuals (positive vs. negative)
res<-ols$residuals
resid<-factor(cut(res, breaks=c(-200, 0, 200), labels=c("negative","positive")))
joincount.test(resid, cont.listw)
```

We can see from the results that negative residuals has spatial correlation from other hand we don't have the statistically proof to say the same thing for the positive values. And we decided to use spatial models because according to Moran test there is a spatial correlation between residuals and there is spatial correlation between negative ones as well.

```{r echo=FALSE, message=FALSE, warning=FALSE}

# spatial distribution of OLS residuals
summary(ols$residuals)
res<-ols$residuals
brks<-c(min(res), mean(res)-sd(res), mean(res), mean(res)+sd(res), max(res))
cols<-c("steelblue4","lightskyblue","thistle1","plum3")
plot(spadata, col=cols[findInterval(res,brks)])
#plot(woj, add=TRUE, lwd=2)
title(main="Spatial Distribution of OLS Residuals")
legend("bottomleft", legend=c("<mean-sd", "(mean-sd, mean)", "(mean, mean+sd)", ">mean+sd"), leglabs(brks1), fill=cols, bty="n")


```


```{r echo=FALSE}
# test join.count for residuals (positive vs. negative)
resid<-factor(cut(res, breaks=c(-200, 0, 200), labels=c("negative","positive")))
joincount.test(resid, cont.listw)
```



Emin degilim buna gerek var mi ama dursun simdilik burada :D
 ```{r}
# # OLS model
# model<-glm(DUI1802 ~ SALESPC + COLLENRP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP, data = spadata)
# summary(model)
# ```
# ```{r}
# SRMSE<-sqrt(sum((model$fitted.values-spadata$DUI1802)^ 2)/ dim(spadata)[1]) / (mean(spadata$DUI1802))
# SRMSE
# ```

# ```{r}
# plot(spadata$SALESPC, spadata$DUI1802)
# points(spadata$SALESPC, model$fitted.values, col="red")
# abline(h=1, lty=3)
# title(main="OLS, multinominal, fitted values")
 ```

what next...

***

### 2.3. SAR /SEM 


GNS: Y= \rho WY+Xβ+WX\theta+u and u= \lambda Wu+e

rho spatial lag of dependent variable (impacts)
beta 
theta spatial lag of explanotary variable
lambda spatial lag of error term (in order to get random term which is epsilon)

We wanted to start with Monski model to see what we should expect from our model.

### 2.4. Monski Model

```{r echo=FALSE, message=FALSE, warning=FALSE}

# Estimation of spatial models with 3,2, or 1 spatial components

# Manski model (full specification) - includes spatial lag of Y (rho), 
# spatial lag of X (theta), spatial error term (lambda)
# option type="sacmixed" activates spatial lags of X
GNS_1<-sacsarlm(SALESPC ~ DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP + FVPTHH02, data = spadata, listw=cont.listw, type="sacmixed", method="LU")  # method="LU" speeds up computations
summary(GNS_1)
```


SOMECOLLP, BKGRTOABC, BAPTISTSP, BKGRTOMIX, ENTRECP, UNEMPP significant and if it comes to thetas just thetas of BAPTISTSP significant while the rest is insignificant. We can also see from the result that Rho and Lambda are insignificant as well and model's AIC is 2377.8, which we are seeking for the smallest AIC for the best model. Since AIC for lm is bigger than AIC we should continue to search for spatial model.


```{r message=FALSE, warning=FALSE}

# SAC / SARAR model - includes spatial lag of Y, spatial error term
SAC_1<-sacsarlm(SALESPC ~ DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP + FVPTHH02, data = spadata, listw=cont.listw)
summary(SAC_1)
```

As we can see in this model thetas disappeared from result as planned. DUI1802 BKGRTOABC BAPTISTSP BKGRTOMIX ENTRECP UNEMPP significant. And we can see this time rho and lambda are significant as well. AIC: 2375.2 which is smaller than AIC for LM still.


<!-- ## TO-DO: add what to do when rho and lambda significant. -->
<!-- 2021-04-19 Class 08 - Practice of spatial dependency models  20:00 -->


<!-- # SEM - spatial error model -->
<!-- # typically includes spatial error term only (with lambda coefficient) -->
<!-- # option etype="emixed" activates spatial lags of X (with theta coeff.) # what makes spatial Durbin error model -->
```{r message=FALSE, warning=FALSE}
SDEM_1<-errorsarlm(SALESPC ~ DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP + FVPTHH02, data = spadata, listw=cont.listw, etype="emixed") # with spatial lags of X
summary(SDEM_1)
```
```{r message=FALSE, warning=FALSE}
SEM_1<-errorsarlm(SALESPC ~ DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP + FVPTHH02, data = spadata, listw=cont.listw) # no spat-lags of X
summary(SEM_1)
```

<!-- # SAR - spatial lag model -->
<!-- # normally includes spatial lag of Y only (with rho coefficient) -->
<!-- # option type="mixed" activates spatial lags of X (with theta coeff.) -->
```{r message=FALSE, warning=FALSE}
SDM_1<-lagsarlm(SALESPC ~ DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP + FVPTHH02, data = spadata, listw=cont.listw, type="mixed") # with spatial lags of X
summary(SDM_1)
```
```{r message=FALSE, warning=FALSE}
SAR_1<-lagsarlm(SALESPC ~ DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP + FVPTHH02, data = spadata, listw=cont.listw) # no spatial lags of X
summary(SAR_1)
```
<!-- # from errorsarlm() library -->
<!-- # an ‘lm’ model augmented with the spatially lagged RHS variables -->
<!-- # RHS variables – right-hand side variables -->
```{r message=FALSE, warning=FALSE}
SLX_1<-lmSLX(SALESPC ~ DUI1802 + SOMECOLLP + BKGRTOABC + BAPTISTSP + BKGRTOMIX + ENTRECP + UNEMPP + PCI + URBANP + FVPTHH02, data = spadata, listw=cont.listw)
summary(SLX_1)
```

### 2.5. Diagnostic Tests

<!-- # LR (likelihood ratio) test - compares nested restricted model -->
<!-- # H0 – restricted (narrower) model is better -->
<!-- # H1 – unrestricted (wider) model is better -->
<!-- # df in chi2 is the number of restricted parameters -->
```{r message=FALSE, warning=FALSE}
LR.sarlm(GNS_1, SDM_1)
LR.sarlm(GNS_1, SDEM_1)
LR.sarlm(GNS_1, SLX_1)
LR.sarlm(SDM_1, SAR_1)
LR.sarlm(SDM_1, SEM_1)
LR.sarlm(SDM_1, SLX_1)
LR.sarlm(SDEM_1, SLX_1)
#lrtest(model.lm, SLX_1)

```



## 3. CONCLUSION



